{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development of AZ policy training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "import nfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "dbparams = {\n",
    "    'dbname': 'bde',\n",
    "    'port': 5432,\n",
    "    'host': 'yuma.hpc.nrel.gov',\n",
    "    'user': 'rlops',\n",
    "    'password': '***REMOVED***',\n",
    "    'options': f'-c search_path=rl',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the tensorflow dataset from the PostgresQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "import alphazero.config as config\n",
    "\n",
    "def psql_generator():\n",
    "    \"\"\" A python generator to yield rows from the Postgres database. Note, here I'm deffering\n",
    "    the actual parsing of the binary data to a later function, which we can hopefully parallelize.\n",
    "    \n",
    "    The SQL command here selects 100 random game states, selected from the (unique) 100 most recent\n",
    "    games (id is the row-id, always increasing with newer games; gameid is a unique game identifier)\n",
    "    \n",
    "    Essentially when this runs out; it should get re-called to grab new data. \n",
    "    \"\"\"\n",
    "    \n",
    "    param_df = {config.batch_size}\n",
    "    \n",
    "    with psycopg2.connect(**dbparams) as conn:\n",
    "        \n",
    "        logging.info(\"Running SQL query\")\n",
    "        \n",
    "        df = pd.read_sql_query(\"\"\"\n",
    "        select * from (\n",
    "            select distinct on (gameid) id, ranked_reward, data\n",
    "            from rl.stablees_replay\n",
    "            order by gameid, random()) as cte\n",
    "        order by id desc limit %s\n",
    "        \"\"\", conn, params=param_df)\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            yield (row.data.tobytes(), row.ranked_reward)\n",
    "            \n",
    "\n",
    "def parse_binary_data(binary_data, reward):\n",
    "    \"\"\" Use io and numpy to parse the binary data from postgresQL\n",
    "    \"\"\"\n",
    "    with io.BytesIO(binary_data.numpy()) as f:\n",
    "        parsed_data = dict(np.load(f, allow_pickle=True).items())\n",
    "    \n",
    "    # This is something we could talk about; but I'm wondering if the best\n",
    "    # loss function for a boolean reward is a binary crossentropy\n",
    "    if reward == -1:\n",
    "        reward = 0\n",
    "        \n",
    "    visit_probs = parsed_data.pop('visit_probs')\n",
    "    return (parsed_data['atom'], parsed_data['bond'],\n",
    "            parsed_data['connectivity'], int(reward), visit_probs)\n",
    "\n",
    "\n",
    "def parse_data_tf(binary_data, reward):\n",
    "    \"\"\"tf.py_func wants a flat list of outputs, but here we restructure to\n",
    "    keras's desired (inputs, outputs) format\"\"\"\n",
    "    atom, bond, connectivity, reward, visit_probs = tf.py_function(\n",
    "        parse_binary_data, inp=[binary_data, reward], \n",
    "        Tout=[tf.int64, tf.int64, tf.int64, tf.int64, tf.float32])\n",
    "    \n",
    "    # The py_func doesn't provide tensor shapes, and we'll need these for the\n",
    "    # padded batch operation\n",
    "    atom.set_shape([None, None])\n",
    "    bond.set_shape([None, None])\n",
    "    connectivity.set_shape([None, None, 2])\n",
    "    reward.set_shape([])\n",
    "    visit_probs.set_shape([None])        \n",
    "    \n",
    "    return ({'atom': atom, 'bond': bond, 'connectivity': connectivity},\n",
    "            (reward, visit_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "dataset = tf.data.Dataset.from_generator(psql_generator, output_types=(tf.string, tf.float32))\\\n",
    "    .repeat()\\\n",
    "    .shuffle(100)\\\n",
    "    .map(parse_data_tf, num_parallel_calls=tf.data.experimental.AUTOTUNE)\\\n",
    "    .padded_batch(batch_size, \n",
    "    padding_values=({'atom': nfp.zero, 'bond': nfp.zero, 'connectivity': nfp.zero}, (nfp.zero, 0.)))\\\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example dataset outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Running SQL query\n",
      "INFO:root:Running SQL query\n",
      "INFO:root:Running SQL query\n",
      "INFO:root:Running SQL query\n",
      "INFO:root:Running SQL query\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([16, 17, 5])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs, outputs = list(dataset.take(1))[0]\n",
    "inputs['atom'].shape  # batch_size, max_actions_per_node, max_atoms_per_mol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the tensorflow model\n",
    "\n",
    "specifically, we need to handle batches of actions to normalize the prior_logits by parent molecule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alphazero.policy import policy_model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.python.keras.losses import LossFunctionWrapper, losses_utils\n",
    "\n",
    "def kl_with_logits(y_true, y_pred):\n",
    "    \"\"\" It's typically more numerically stable *not* to perform the softmax,\n",
    "    but instead define the loss based on the raw logit predictions. This loss\n",
    "    function corrects a tensorflow omission where there isn't a KLD loss that\n",
    "    accepts raw logits. \"\"\"\n",
    "\n",
    "    # Mask nan values in y_true with zeros\n",
    "    y_true = tf.where(tf.math.is_finite(y_true), y_true, tf.zeros_like(y_true))\n",
    "\n",
    "    return (\n",
    "        tf.keras.losses.categorical_crossentropy(y_true, y_pred, from_logits=True) -\n",
    "        tf.keras.losses.categorical_crossentropy(y_true, y_true, from_logits=False))\n",
    "\n",
    "\n",
    "class KLWithLogits(LossFunctionWrapper):\n",
    "    \"\"\" Keras sometimes wants these loss function wrappers to define how to\n",
    "    reduce the loss over variable batch sizes \"\"\"\n",
    "    def __init__(self,\n",
    "                 reduction=losses_utils.ReductionV2.AUTO,\n",
    "                 name='kl_with_logits'):\n",
    "\n",
    "        super(KLWithLogits, self).__init__(\n",
    "            kl_with_logits,\n",
    "            name=name,\n",
    "            reduction=reduction)\n",
    "    \n",
    "\n",
    "class PolicyWrapper(layers.Layer):\n",
    "    def build(self, input_shape):\n",
    "        self.policy_model = policy_model()\n",
    "        \n",
    "    def call(self, inputs, mask=None):\n",
    "        atom, bond, connectivity = inputs\n",
    "    \n",
    "        # Get the batch and action dimensions\n",
    "        atom_shape = tf.shape(atom)\n",
    "        batch_size = atom_shape[0]\n",
    "        max_actions_per_node = atom_shape[1]\n",
    "        \n",
    "        # Flatten the inputs for running individually through the policy model\n",
    "        atom_flat = tf.reshape(atom, [batch_size * max_actions_per_node, -1])\n",
    "        bond_flat = tf.reshape(bond, [batch_size * max_actions_per_node, -1])\n",
    "        connectivity_flat = tf.reshape(connectivity, [batch_size * max_actions_per_node, -1, 2])\n",
    "\n",
    "        # Get the flat value and prior_logit predictions\n",
    "        flat_values, flat_prior_logits = self.policy_model([atom_flat, bond_flat, connectivity_flat])      \n",
    "        \n",
    "        # We put the parent node first in our batch inputs, so this slices\n",
    "        # the value prediction for the parent\n",
    "        value_preds = tf.reshape(flat_values, [batch_size, max_actions_per_node, -1])[:, 0, 0]\n",
    "        \n",
    "        # Next we get a mask to see where we have valid actions and replace priors for\n",
    "        # invalid actions with negative infinity (these get zeroed out after softmax).\n",
    "        # We also only return prior_logits for the child nodes (not the first entry)\n",
    "        action_mask = tf.reduce_any(tf.not_equal(atom, 0), axis=-1)  # zero is the padding element\n",
    "        prior_logits = tf.reshape(flat_prior_logits, [batch_size, max_actions_per_node])\n",
    "        masked_prior_logits = tf.where(action_mask, prior_logits,\n",
    "                                       tf.ones_like(prior_logits) * prior_logits.dtype.min)[:, 1:]\n",
    "        \n",
    "        return value_preds, masked_prior_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we actually build the tf.keras.Model to train\n",
    "\n",
    "atom_class = layers.Input(shape=[None, None], dtype=tf.int64, name='atom')\n",
    "bond_class = layers.Input(shape=[None, None], dtype=tf.int64, name='bond')\n",
    "connectivity = layers.Input(shape=[None, None, 2], dtype=tf.int64, name='connectivity')\n",
    "\n",
    "value_preds, masked_prior_logits = PolicyWrapper()([atom_class, bond_class, connectivity])\n",
    "\n",
    "policy_trainer = tf.keras.Model([atom_class, bond_class, connectivity], [value_preds, masked_prior_logits])\n",
    "\n",
    "policy_trainer.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1E-4),  # Do AZ list their optimizer?\n",
    "    loss=[tf.keras.losses.BinaryCrossentropy(), KLWithLogits()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "note the losses indeed decrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Running SQL query\n",
      "INFO:root:Running SQL query\n",
      "INFO:root:Running SQL query\n",
      "INFO:root:Running SQL query\n",
      "INFO:root:Running SQL query\n",
      "INFO:root:Running SQL query\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4/50 [=>............................] - ETA: 0s - loss: 4.6876 - policy_wrapper_loss: 3.8955 - policy_wrapper_1_loss: 0.7921"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Running SQL query\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 6/50 [==>...........................] - ETA: 1s - loss: 4.4949 - policy_wrapper_loss: 3.6454 - policy_wrapper_1_loss: 0.8495"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Running SQL query\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 8/50 [===>..........................] - ETA: 1s - loss: 4.3616 - policy_wrapper_loss: 3.5114 - policy_wrapper_1_loss: 0.8502"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Running SQL query\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "10/50 [=====>........................] - ETA: 1s - loss: 4.2544 - policy_wrapper_loss: 3.4019 - policy_wrapper_1_loss: 0.8525"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Running SQL query\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/50 [======>.......................] - ETA: 1s - loss: 4.1021 - policy_wrapper_loss: 3.2706 - policy_wrapper_1_loss: 0.8315"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Running SQL query\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "15/50 [========>.....................] - ETA: 1s - loss: 4.0172 - policy_wrapper_loss: 3.1990 - policy_wrapper_1_loss: 0.8182"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Running SQL query\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "16/50 [========>.....................] - ETA: 1s - loss: 3.9927 - policy_wrapper_loss: 3.1629 - policy_wrapper_1_loss: 0.8297"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Running SQL query\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "18/50 [=========>....................] - ETA: 1s - loss: 3.9279 - policy_wrapper_loss: 3.0957 - policy_wrapper_1_loss: 0.8322"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Running SQL query\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "20/50 [===========>..................] - ETA: 1s - loss: 3.8716 - policy_wrapper_loss: 3.0364 - policy_wrapper_1_loss: 0.8352"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Running SQL query\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "22/50 [============>.................] - ETA: 1s - loss: 3.8146 - policy_wrapper_loss: 2.9843 - policy_wrapper_1_loss: 0.8303"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Running SQL query\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "24/50 [=============>................] - ETA: 1s - loss: 3.7606 - policy_wrapper_loss: 2.9360 - policy_wrapper_1_loss: 0.8246"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Running SQL query\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "26/50 [==============>...............] - ETA: 0s - loss: 3.7118 - policy_wrapper_loss: 2.8902 - policy_wrapper_1_loss: 0.8216"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Running SQL query\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "28/50 [===============>..............] - ETA: 0s - loss: 3.6650 - policy_wrapper_loss: 2.8462 - policy_wrapper_1_loss: 0.8188"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Running SQL query\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "30/50 [=================>............] - ETA: 0s - loss: 3.6272 - policy_wrapper_loss: 2.8063 - policy_wrapper_1_loss: 0.8208"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Running SQL query\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/50 [==================>...........] - ETA: 0s - loss: 3.5867 - policy_wrapper_loss: 2.7681 - policy_wrapper_1_loss: 0.8187"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Running SQL query\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "34/50 [===================>..........] - ETA: 0s - loss: 3.5550 - policy_wrapper_loss: 2.7349 - policy_wrapper_1_loss: 0.8201"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Running SQL query\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "36/50 [====================>.........] - ETA: 0s - loss: 3.5122 - policy_wrapper_loss: 2.7012 - policy_wrapper_1_loss: 0.8110"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Running SQL query\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "38/50 [=====================>........] - ETA: 0s - loss: 3.4763 - policy_wrapper_loss: 2.6681 - policy_wrapper_1_loss: 0.8082"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Running SQL query\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "40/50 [=======================>......] - ETA: 0s - loss: 3.4472 - policy_wrapper_loss: 2.6360 - policy_wrapper_1_loss: 0.8113"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Running SQL query\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "42/50 [========================>.....] - ETA: 0s - loss: 3.4197 - policy_wrapper_loss: 2.6083 - policy_wrapper_1_loss: 0.8114"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Running SQL query\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "44/50 [=========================>....] - ETA: 0s - loss: 3.3928 - policy_wrapper_loss: 2.5800 - policy_wrapper_1_loss: 0.8129"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Running SQL query\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "46/50 [==========================>...] - ETA: 0s - loss: 3.3666 - policy_wrapper_loss: 2.5531 - policy_wrapper_1_loss: 0.8135"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Running SQL query\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "48/50 [===========================>..] - ETA: 0s - loss: 3.3421 - policy_wrapper_loss: 2.5258 - policy_wrapper_1_loss: 0.8164"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Running SQL query\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 2s 43ms/step - loss: 3.3155 - policy_wrapper_loss: 2.5016 - policy_wrapper_1_loss: 0.8139\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f0c3cdceed0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, with the log-level showing when the SQL buffer is refilled\n",
    "policy_trainer.fit(dataset, steps_per_epoch=50, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "500/500 [==============================] - 20s 40ms/step - loss: 1.2958 - policy_wrapper_loss: 0.5023 - policy_wrapper_1_loss: 0.7935\n",
      "Epoch 2/10\n",
      "500/500 [==============================] - 20s 40ms/step - loss: 0.6097 - policy_wrapper_loss: 0.0265 - policy_wrapper_1_loss: 0.5832\n",
      "Epoch 3/10\n",
      "500/500 [==============================] - 20s 40ms/step - loss: 0.5328 - policy_wrapper_loss: 0.0066 - policy_wrapper_1_loss: 0.5261\n",
      "Epoch 4/10\n",
      "500/500 [==============================] - 20s 41ms/step - loss: 0.4804 - policy_wrapper_loss: 0.0024 - policy_wrapper_1_loss: 0.4780\n",
      "Epoch 5/10\n",
      "500/500 [==============================] - 20s 40ms/step - loss: 0.4410 - policy_wrapper_loss: 0.0010 - policy_wrapper_1_loss: 0.4399\n",
      "Epoch 6/10\n",
      "500/500 [==============================] - 19s 39ms/step - loss: 0.4160 - policy_wrapper_loss: 5.8499e-04 - policy_wrapper_1_loss: 0.4154\n",
      "Epoch 7/10\n",
      "500/500 [==============================] - 20s 40ms/step - loss: 0.3842 - policy_wrapper_loss: 3.6488e-04 - policy_wrapper_1_loss: 0.3838\n",
      "Epoch 8/10\n",
      "500/500 [==============================] - 21s 42ms/step - loss: 0.3671 - policy_wrapper_loss: 2.3797e-04 - policy_wrapper_1_loss: 0.3668\n",
      "Epoch 9/10\n",
      "500/500 [==============================] - 20s 41ms/step - loss: 0.3581 - policy_wrapper_loss: 1.7967e-04 - policy_wrapper_1_loss: 0.3579\n",
      "Epoch 10/10\n",
      "500/500 [==============================] - 20s 41ms/step - loss: 0.3459 - policy_wrapper_loss: 1.3442e-04 - policy_wrapper_1_loss: 0.3458\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f0c3d094950>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Next, turn off the logger and train for a longer period\n",
    "logging.getLogger().setLevel(logging.WARN)\n",
    "policy_trainer.fit(dataset, steps_per_epoch=500, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A concern here is that the value prediction is trivial in the case -- every game 'succeeds', so there's not examples of bad choices. This is where we need ranked rewards (eventually)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to get the weights from the trainied model\n",
    "\n",
    "we'll probably need to load the policy_trainer, and then extract the policy_model sub-model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = policy_model()\n",
    "policy.set_weights(policy_trainer.layers[-1].policy_model.get_weights())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
