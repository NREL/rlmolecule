{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development of AZ policy training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "import nfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "dbparams = {\n",
    "    'dbname': 'bde',\n",
    "    'port': 5432,\n",
    "    'host': 'yuma.hpc.nrel.gov',\n",
    "    'user': 'rlops',\n",
    "    'password': 'jTeL85L!',\n",
    "    'options': f'-c search_path=rl',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdkit\n",
    "from rdkit import Chem\n",
    "from rdkit import DataStructs\n",
    "\n",
    "radicals = pd.read_csv('/projects/rlmolecule/pstjohn/q2_milestone/radicals.csv.gz')['0']\n",
    "radical_fps = pd.read_pickle('/projects/rlmolecule/pstjohn/q2_milestone/binary_fps.p.gz').apply(\n",
    "    DataStructs.CreateFromBinaryText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_fp = Chem.RDKFingerprint(rdkit.Chem.MolFromSmiles('COOO'))\n",
    "max(DataStructs.BulkTanimotoSimilarity(target_fp, radical_fps.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alphazero.config import AlphaZeroConfig\n",
    "CONFIG = AlphaZeroConfig()\n",
    "\n",
    "sq = \"\"\"\n",
    "        select percentile_cont(%s) within group (order by reward) from (\n",
    "            select reward \n",
    "            from rl.q2replayerotokritos \n",
    "            order by id desc limit %s) as finals  \n",
    "        \"\"\"\n",
    "param = {CONFIG.ranked_reward_alpha, CONFIG.batch_size}\n",
    "\n",
    "with psycopg2.connect(**dbparams) as conn:        \n",
    "        r_alpha = pd.read_sql_query(sq, conn, params=param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_alpha['percentile_cont'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the tensorflow dataset from the PostgresQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "from alphazero.config import AlphaZeroConfig\n",
    "CONFIG = AlphaZeroConfig()\n",
    "\n",
    "def psql_generator():\n",
    "    \"\"\" A python generator to yield rows from the Postgres database. Note, here I'm deffering\n",
    "    the actual parsing of the binary data to a later function, which we can hopefully parallelize.\n",
    "    \n",
    "    The SQL command here selects 100 random game states, selected from the (unique) 100 most recent\n",
    "    games (id is the row-id, always increasing with newer games; gameid is a unique game identifier)\n",
    "    \n",
    "    Essentially when this runs out; it should get re-called to grab new data. \n",
    "    \"\"\"\n",
    "    \n",
    "    param_df = {CONFIG.batch_size}\n",
    "    param = {CONFIG.ranked_reward_alpha, CONFIG.batch_size}\n",
    "    \n",
    "    with psycopg2.connect(**dbparams) as conn:\n",
    "        \n",
    "        logging.info(\"Running SQL query\")\n",
    "        \n",
    "        df = pd.read_sql_query(\"\"\"\n",
    "        select * from (\n",
    "            select distinct on (gameid) id, binary_reward, reward, data\n",
    "            from rl.q2replayerotokritos\n",
    "            order by gameid, random()) as cte\n",
    "        order by id desc limit %s\n",
    "        \"\"\", conn, params=param_df)\n",
    "        \n",
    "        r_alpha = pd.read_sql_query(\"\"\"\n",
    "        select percentile_cont(%s) within group (order by reward) from (\n",
    "            select reward \n",
    "            from rl.q2replayerotokritos \n",
    "            order by id desc limit %s) as finals  \n",
    "        \"\"\", conn, params=param)\n",
    "        \n",
    "        # The following calculates the ranked reward based on max_similarity scores\n",
    "        for _, row in df.iterrows():\n",
    "            if row.reward > r_alpha['percentile_cont'][0]:\n",
    "                row.binary_reward = 1.\n",
    "            elif row.reward < r_alpha['percentile_cont'][0]:\n",
    "                row.binary_reward = -1.\n",
    "            else:\n",
    "                row.binary_reward = np.random.choice([-1.,1.])\n",
    "            yield (row.data.tobytes(), row.binary_reward)\n",
    "            \n",
    "\n",
    "def parse_binary_data(binary_data, reward):\n",
    "    \"\"\" Use io and numpy to parse the binary data from postgresQL\n",
    "    \"\"\"\n",
    "    with io.BytesIO(binary_data.numpy()) as f:\n",
    "        parsed_data = dict(np.load(f, allow_pickle=True).items())\n",
    "    \n",
    "    # This is something we could talk about; but I'm wondering if the best\n",
    "    # loss function for a boolean reward is a binary crossentropy\n",
    "    if reward == -1:\n",
    "        reward = 0\n",
    "        \n",
    "    visit_probs = parsed_data.pop('visit_probs')\n",
    "    return (parsed_data['atom'], parsed_data['bond'],\n",
    "            parsed_data['connectivity'], int(reward), visit_probs)\n",
    "\n",
    "\n",
    "def parse_data_tf(binary_data, reward):\n",
    "    \"\"\"tf.py_func wants a flat list of outputs, but here we restructure to\n",
    "    keras's desired (inputs, outputs) format\"\"\"\n",
    "    atom, bond, connectivity, reward, visit_probs = tf.py_function(\n",
    "        parse_binary_data, inp=[binary_data, reward], \n",
    "        Tout=[tf.int64, tf.int64, tf.int64, tf.int64, tf.float32])\n",
    "    \n",
    "    # The py_func doesn't provide tensor shapes, and we'll need these for the\n",
    "    # padded batch operation\n",
    "    atom.set_shape([None, None])\n",
    "    bond.set_shape([None, None])\n",
    "    connectivity.set_shape([None, None, 2])\n",
    "    reward.set_shape([])\n",
    "    visit_probs.set_shape([None])        \n",
    "    \n",
    "    return ({'atom': atom, 'bond': bond, 'connectivity': connectivity},\n",
    "            (reward, visit_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "dataset = tf.data.Dataset.from_generator(psql_generator, output_types=(tf.string, tf.float32))\\\n",
    "    .repeat()\\\n",
    "    .shuffle(100)\\\n",
    "    .map(parse_data_tf, num_parallel_calls=tf.data.experimental.AUTOTUNE)\\\n",
    "    .padded_batch(batch_size, \n",
    "    padding_values=({'atom': nfp.zero, 'bond': nfp.zero, 'connectivity': nfp.zero}, (nfp.zero, 0.)))\\\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example dataset outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, outputs = list(dataset.take(1))[0]\n",
    "inputs['atom'].shape  # batch_size, max_actions_per_node, max_atoms_per_mol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the tensorflow model\n",
    "\n",
    "specifically, we need to handle batches of actions to normalize the prior_logits by parent molecule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alphazero.policy import policy_model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.python.keras.losses import LossFunctionWrapper, losses_utils\n",
    "\n",
    "def kl_with_logits(y_true, y_pred):\n",
    "    \"\"\" It's typically more numerically stable *not* to perform the softmax,\n",
    "    but instead define the loss based on the raw logit predictions. This loss\n",
    "    function corrects a tensorflow omission where there isn't a KLD loss that\n",
    "    accepts raw logits. \"\"\"\n",
    "\n",
    "    # Mask nan values in y_true with zeros\n",
    "    y_true = tf.where(tf.math.is_finite(y_true), y_true, tf.zeros_like(y_true))\n",
    "\n",
    "    return (\n",
    "        tf.keras.losses.categorical_crossentropy(y_true, y_pred, from_logits=True) -\n",
    "        tf.keras.losses.categorical_crossentropy(y_true, y_true, from_logits=False))\n",
    "\n",
    "\n",
    "class KLWithLogits(LossFunctionWrapper):\n",
    "    \"\"\" Keras sometimes wants these loss function wrappers to define how to\n",
    "    reduce the loss over variable batch sizes \"\"\"\n",
    "    def __init__(self,\n",
    "                 reduction=losses_utils.ReductionV2.AUTO,\n",
    "                 name='kl_with_logits'):\n",
    "\n",
    "        super(KLWithLogits, self).__init__(\n",
    "            kl_with_logits,\n",
    "            name=name,\n",
    "            reduction=reduction)\n",
    "    \n",
    "\n",
    "class PolicyWrapper(layers.Layer):\n",
    "    def build(self, input_shape):\n",
    "        self.policy_model = policy_model()\n",
    "        \n",
    "    def call(self, inputs, mask=None):\n",
    "        atom, bond, connectivity = inputs\n",
    "    \n",
    "        # Get the batch and action dimensions\n",
    "        atom_shape = tf.shape(atom)\n",
    "        batch_size = atom_shape[0]\n",
    "        max_actions_per_node = atom_shape[1]\n",
    "        \n",
    "        # Flatten the inputs for running individually through the policy model\n",
    "        atom_flat = tf.reshape(atom, [batch_size * max_actions_per_node, -1])\n",
    "        bond_flat = tf.reshape(bond, [batch_size * max_actions_per_node, -1])\n",
    "        connectivity_flat = tf.reshape(connectivity, [batch_size * max_actions_per_node, -1, 2])\n",
    "\n",
    "        # Get the flat value and prior_logit predictions\n",
    "        flat_values, flat_prior_logits = self.policy_model([atom_flat, bond_flat, connectivity_flat])      \n",
    "        \n",
    "        # We put the parent node first in our batch inputs, so this slices\n",
    "        # the value prediction for the parent\n",
    "        value_preds = tf.reshape(flat_values, [batch_size, max_actions_per_node, -1])[:, 0, 0]\n",
    "        \n",
    "        # Next we get a mask to see where we have valid actions and replace priors for\n",
    "        # invalid actions with negative infinity (these get zeroed out after softmax).\n",
    "        # We also only return prior_logits for the child nodes (not the first entry)\n",
    "        action_mask = tf.reduce_any(tf.not_equal(atom, 0), axis=-1)  # zero is the padding element\n",
    "        prior_logits = tf.reshape(flat_prior_logits, [batch_size, max_actions_per_node])\n",
    "        masked_prior_logits = tf.where(action_mask, prior_logits,\n",
    "                                       tf.ones_like(prior_logits) * prior_logits.dtype.min)[:, 1:]\n",
    "        \n",
    "        return value_preds, masked_prior_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we actually build the tf.keras.Model to train\n",
    "\n",
    "atom_class = layers.Input(shape=[None, None], dtype=tf.int64, name='atom')\n",
    "bond_class = layers.Input(shape=[None, None], dtype=tf.int64, name='bond')\n",
    "connectivity = layers.Input(shape=[None, None, 2], dtype=tf.int64, name='connectivity')\n",
    "\n",
    "value_preds, masked_prior_logits = PolicyWrapper()([atom_class, bond_class, connectivity])\n",
    "\n",
    "policy_trainer = tf.keras.Model([atom_class, bond_class, connectivity], [value_preds, masked_prior_logits])\n",
    "\n",
    "policy_trainer.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1E-4),  # Do AZ list their optimizer?\n",
    "    loss=[tf.keras.losses.BinaryCrossentropy(), KLWithLogits()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "note the losses indeed decrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, with the log-level showing when the SQL buffer is refilled\n",
    "policy_trainer.fit(dataset, steps_per_epoch=50, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, turn off the logger and train for a longer period\n",
    "logging.getLogger().setLevel(logging.WARN)\n",
    "policy_trainer.fit(dataset, steps_per_epoch=500, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A concern here is that the value prediction is trivial in the case -- every game 'succeeds', so there's not examples of bad choices. This is where we need ranked rewards (eventually)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to get the weights from the trainied model\n",
    "\n",
    "we'll probably need to load the policy_trainer, and then extract the policy_model sub-model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = policy_model()\n",
    "policy.set_weights(policy_trainer.layers[-1].policy_model.get_weights())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
