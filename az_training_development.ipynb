{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development of AZ policy training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "import nfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "dbparams = {\n",
    "    'dbname': 'bde',\n",
    "    'port': 5432,\n",
    "    'host': 'yuma.hpc.nrel.gov',\n",
    "    'user': 'rlops',\n",
    "    'password': '***REMOVED***',\n",
    "    'options': f'-c search_path=rl',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the tensorflow dataset from the PostgresQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "\n",
    "def psql_generator():\n",
    "    with psycopg2.connect(**dbparams) as conn:\n",
    "        df = pd.read_sql_query(\"\"\"\n",
    "        select * from (\n",
    "            select distinct on (gameid) id, reward, data\n",
    "            from rl.q2replay\n",
    "            order by gameid, random()) as cte\n",
    "        order by id desc limit 100\n",
    "        \"\"\", conn)\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            yield (row.data.tobytes(), row.reward)\n",
    "            \n",
    "\n",
    "def parse_binary_data(binary_data, reward):\n",
    "    \"\"\" Use io and numpy to parse the binary data from postgresQL\n",
    "    \"\"\"\n",
    "    with io.BytesIO(binary_data.numpy()) as f:\n",
    "        parsed_data = dict(np.load(f, allow_pickle=True).items())\n",
    "    \n",
    "    # This is something we could talk about; but I'm wondering if the best\n",
    "    # loss function for a boolean reward is a binary crossentropy\n",
    "    if reward == -1:\n",
    "        reward = 0\n",
    "        \n",
    "    visit_probs = parsed_data.pop('visit_probs')\n",
    "    return (parsed_data['atom'], parsed_data['bond'],\n",
    "            parsed_data['connectivity'], int(reward), visit_probs)\n",
    "\n",
    "\n",
    "def parse_data_tf(binary_data, reward):\n",
    "    \"\"\"tf.py_func wants a flat list of outputs, but here we restructure to\n",
    "    keras's desired (inputs, outputs) format\"\"\"\n",
    "    atom, bond, connectivity, reward, visit_probs = tf.py_function(\n",
    "        parse_binary_data, inp=[binary_data, reward], \n",
    "        Tout=[tf.int64, tf.int64, tf.int64, tf.int64, tf.float32])\n",
    "    \n",
    "    # The py_func doesn't provide tensor shapes, and we'll need these for the\n",
    "    # padded batch operation\n",
    "    atom.set_shape([None, None])\n",
    "    bond.set_shape([None, None])\n",
    "    connectivity.set_shape([None, None, 2])\n",
    "    reward.set_shape([])\n",
    "    visit_probs.set_shape([None])        \n",
    "    \n",
    "    return ({'atom': atom, 'bond': bond, 'connectivity': connectivity},\n",
    "            (reward, visit_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "dataset = tf.data.Dataset.from_generator(psql_generator, output_types=(tf.string, tf.float32))\\\n",
    "    .repeat()\\\n",
    "    .shuffle(100)\\\n",
    "    .map(parse_data_tf, num_parallel_calls=tf.data.experimental.AUTOTUNE)\\\n",
    "    .padded_batch(batch_size, \n",
    "    padding_values=({'atom': nfp.zero, 'bond': nfp.zero, 'connectivity': nfp.zero}, (nfp.zero, 0.)))\\\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example dataset outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([16, 23, 4])"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs, outputs = list(dataset.take(1))[0]\n",
    "inputs['atom'].shape  # batch_size, max_actions_per_node, max_atoms_per_mol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the tensorflow model\n",
    "\n",
    "specifically, we need to handle batches of actions to normalize the prior_logits by parent molecule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alphazero.policy import policy_model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.python.keras.losses import LossFunctionWrapper, losses_utils\n",
    "\n",
    "def kl_with_logits(y_true, y_pred):\n",
    "    \"\"\" It's typically more numerically stable *not* to perform the softmax,\n",
    "    but instead define the loss based on the raw logit predictions. This loss\n",
    "    function corrects a tensorflow omission where there isn't a KLD loss that\n",
    "    accepts raw logits. \"\"\"\n",
    "\n",
    "    # Mask nan values in y_true with zeros\n",
    "    y_true = tf.where(tf.math.is_finite(y_true), y_true, tf.zeros_like(y_true))\n",
    "\n",
    "    return (\n",
    "        tf.keras.losses.categorical_crossentropy(y_true, y_pred, from_logits=True) -\n",
    "        tf.keras.losses.categorical_crossentropy(y_true, y_true, from_logits=False))\n",
    "\n",
    "\n",
    "class KLWithLogits(LossFunctionWrapper):\n",
    "    \"\"\" Keras sometimes wants these loss function wrappers to define how to\n",
    "    reduce the loss over variable batch sizes \"\"\"\n",
    "    def __init__(self,\n",
    "                 reduction=losses_utils.ReductionV2.AUTO,\n",
    "                 name='kl_with_logits'):\n",
    "\n",
    "        super(KLWithLogits, self).__init__(\n",
    "            kl_with_logits,\n",
    "            name=name,\n",
    "            reduction=reduction)\n",
    "    \n",
    "\n",
    "class PolicyWrapper(layers.Layer):\n",
    "    def build(self, input_shape):\n",
    "        self.policy_model = policy_model()\n",
    "        \n",
    "    def call(self, inputs, mask=None):\n",
    "        atom, bond, connectivity = inputs\n",
    "    \n",
    "        # Get the batch and action dimensions\n",
    "        atom_shape = tf.shape(atom)\n",
    "        batch_size = atom_shape[0]\n",
    "        max_actions_per_node = atom_shape[1]\n",
    "        \n",
    "        # Flatten the inputs for running individually through the policy model\n",
    "        atom_flat = tf.reshape(atom, [batch_size * max_actions_per_node, -1])\n",
    "        bond_flat = tf.reshape(bond, [batch_size * max_actions_per_node, -1])\n",
    "        connectivity_flat = tf.reshape(connectivity, [batch_size * max_actions_per_node, -1, 2])\n",
    "\n",
    "        # Get the flat value and prior_logit predictions\n",
    "        flat_values, flat_prior_logits = self.policy_model([atom_flat, bond_flat, connectivity_flat])      \n",
    "        \n",
    "        # We put the parent node first in our batch inputs, so this slices\n",
    "        # the value prediction for the parent\n",
    "        value_preds = tf.reshape(flat_values, [batch_size, max_actions_per_node, -1])[:, 0, 0]\n",
    "        \n",
    "        # Next we get a mask to see where we have valid actions and replace priors for\n",
    "        # invalid actions with negative infinity (these get zeroed out after softmax).\n",
    "        # We also only return prior_logits for the child nodes (not the first entry)\n",
    "        action_mask = tf.reduce_any(tf.not_equal(atom, 0), axis=-1)  # zero is the padding element\n",
    "        prior_logits = tf.reshape(flat_prior_logits, [batch_size, max_actions_per_node])\n",
    "        masked_prior_logits = tf.where(action_mask, prior_logits,\n",
    "                                       tf.ones_like(prior_logits) * prior_logits.dtype.min)[:, 1:]\n",
    "        \n",
    "        return value_preds, masked_prior_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we actually build the tf.keras.Model to train\n",
    "\n",
    "atom_class = layers.Input(shape=[None, None], dtype=tf.int64, name='atom')\n",
    "bond_class = layers.Input(shape=[None, None], dtype=tf.int64, name='bond')\n",
    "connectivity = layers.Input(shape=[None, None, 2], dtype=tf.int64, name='connectivity')\n",
    "\n",
    "value_preds, masked_prior_logits = PolicyWrapper()([atom_class, bond_class, connectivity])\n",
    "\n",
    "policy_trainer = tf.keras.Model([atom_class, bond_class, connectivity], [value_preds, masked_prior_logits])\n",
    "\n",
    "policy_trainer.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1E-4),  # Do AZ list their optimizer?\n",
    "    loss=[tf.keras.losses.BinaryCrossentropy(), KLWithLogits()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "note the losses indeed decrease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "500/500 [==============================] - 21s 41ms/step - loss: 1.5237 - policy_wrapper_7_loss: 0.6827 - policy_wrapper_7_1_loss: 0.8410\n",
      "Epoch 2/10\n",
      "500/500 [==============================] - 19s 38ms/step - loss: 0.6094 - policy_wrapper_7_loss: 0.0198 - policy_wrapper_7_1_loss: 0.5896\n",
      "Epoch 3/10\n",
      "500/500 [==============================] - 22s 45ms/step - loss: 0.4439 - policy_wrapper_7_loss: 0.0058 - policy_wrapper_7_1_loss: 0.4381\n",
      "Epoch 4/10\n",
      "500/500 [==============================] - 20s 41ms/step - loss: 0.3671 - policy_wrapper_7_loss: 0.0033 - policy_wrapper_7_1_loss: 0.3638\n",
      "Epoch 5/10\n",
      "500/500 [==============================] - 20s 40ms/step - loss: 0.2805 - policy_wrapper_7_loss: 0.0023 - policy_wrapper_7_1_loss: 0.2783\n",
      "Epoch 6/10\n",
      "500/500 [==============================] - 20s 39ms/step - loss: 0.2455 - policy_wrapper_7_loss: 0.0014 - policy_wrapper_7_1_loss: 0.2441 11s - loss - ETA: 6s \n",
      "Epoch 7/10\n",
      "500/500 [==============================] - 18s 37ms/step - loss: 0.2299 - policy_wrapper_7_loss: 8.9306e-04 - policy_wrapper_7_1_loss: 0.2291\n",
      "Epoch 8/10\n",
      "500/500 [==============================] - 19s 39ms/step - loss: 0.2206 - policy_wrapper_7_loss: 5.7860e-04 - policy_wrapper_7_1_loss: 0.2200 3s - loss: 0.2220 - policy_wrapper_7_loss: 5.9610e\n",
      "Epoch 9/10\n",
      "500/500 [==============================] - 19s 39ms/step - loss: 0.2104 - policy_wrapper_7_loss: 3.7562e-04 - policy_wrapper_7_1_loss: 0.2100\n",
      "Epoch 10/10\n",
      "500/500 [==============================] - 18s 37ms/step - loss: 0.2026 - policy_wrapper_7_loss: 2.7534e-04 - policy_wrapper_7_1_loss: 0.2024\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbdbda52c50>"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_trainer.fit(dataset, steps_per_epoch=500, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to get the weights from the trainied model\n",
    "\n",
    "we'll probably need to load the policy_trainer, and then extract the policy_model sub-model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = policy_model()\n",
    "policy.set_weights(policy_trainer.layers[-1].policy_model.get_weights())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
