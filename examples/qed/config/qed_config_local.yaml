# Optimize the Quantitative Estimate of Drug-likeness (QED)
# See https://www.rdkit.org/docs/source/rdkit.Chem.QED.html
# Starting point: a single carbon (C)
#   - actions: add a bond or an atom
#   - state: molecule state
#   - reward: 0, unless a terminal state is reached, then the qed estimate of the molecule

run_id: 'qed_example_local'

# Parameters for setting up the problem
problem_config:
  # maximum number of heavy atoms
  max_atoms: 10
  # minimum number of heavy atoms
  min_atoms: 4
  # atoms to use when building the molecule
  atom_additions: [ 'C', 'N', 'O' ]
  # if set, don't construct molecules greater than a given Synthetic Accessibility (SA) score
  # see: https://github.com/rdkit/rdkit/blob/master/Contrib/SA_Score/sascorer.py
  sa_score_threshold: 3.5
  # whether to consider stereoisomers different molecules
  stereoisomers: False
  # try to get a 3D embedding of the molecule, and if this fails, remote it.
  try_embedding: False
  # Amount of time (in seconds) to run the multiprocessing jobs
  timeout: 300

# Parameters for training the policy model
train_config:
  # Reward options:
  # if the reward for a given game is > the previous
  # *ranked_reward_alpha* fraction of games (e.g., 75% of games),
  # then it's a win. Otherwise, it's a loss.
  ranked_reward_alpha: 0.75
  # max/min number of games to consider
  reward_buffer_max_size: 500
  reward_buffer_min_size: 10

  # Learning options:
  # some useful tips for selecting these parameter values:
  # https://stackoverflow.com/a/49924566/7483950
  # learning rate
  lr: 1E-3
  # number times that the learning algorithm will work through the entire training dataset
  epochs: 1E4
  # number of batch iterations before a training epoch is considered finished
  steps_per_epoch: 100
  # number of seconds to wait to check if enough games have been played
  game_count_delay: 20
  verbose: 2

  # AlphaZero problem options
  # max/min number of games to consider (ordered by time) when training the policy
  max_buffer_size: 200
  min_buffer_size: 15
  # number of training examples to use when updating model parameters
  batch_size: 32
  # folder in which to store the trained models
  policy_checkpoint_dir: 'policy_checkpoints_local'

  # MoleculeTFAlphaZeroProblem options
  # size of network hidden layers
  features: 8
  # number of global state attention heads. Must be a factor of `features`
  num_heads: 2
  # number of message passing layers
  num_messages: 1

# Parameters for running the Monte Carlo Tree Search games
mcts_config:
  # Minimum reward to return for invalid actions
  min_reward: 0
  pbc_c_base: 1.0
  pbc_c_init: 1.25
  # dirichlet 'shape' parameter. Larger values spread out probability over more moves.
  dirichlet_alpha: 1.0
  # percentage to favor dirichlet noise vs. prior estimation. Smaller means less noise
  dirichlet_x: 0.25
  # number of samples to perform at each level of the MCTS search
  num_mcts_samples: 100
  # the maximum search depth.
  max_depth: 1000000
  #ucb_constant: math.sqrt(2)

# Database settings for the Object Relational Model (ORM)
# Used to store games and communicate between the policy model (run on GPUs) and rollout (run on CPUs)
sql_database:
  # default database settings:
  drivername: "sqlite"
  db_file: "qed_data.db"

