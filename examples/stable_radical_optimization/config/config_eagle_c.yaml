# Config file for stable_radical_opt.py

run_id: 'stable_rad_c_large'

# Parameters for setting up the problem
problem_config:
  initial_state: 'C'
  # maximum number of heavy atoms
  max_atoms: 15
  # minimum number of heavy atoms
  min_atoms: 4
  # atoms to use when building the molecule
  atom_additions: [ 'C', 'N', 'O', 'S' ]
  # if set, don't construct molecules greater than a given Synthetic Accessibility (SA) score
  # see: https://github.com/rdkit/rdkit/blob/master/Contrib/SA_Score/sascorer.py
  sa_score_threshold: 4.0
  # whether to consider stereoisomers different molecules
  stereoisomers: True
  # try to get a 3D embedding of the molecule, and if this fails, remote it.
  try_embedding: True
  cache_dir: '$LOCAL_SCRATCH/$USER/rlmolecule_cache/'
  num_shards: 18

# Parameters for training the policy model
train_config:
  # Reward options:
  # if the reward for a given game is > the previous
  # *ranked_reward_alpha* fraction of games (e.g., 75% of games),
  # then it's a win. Otherwise, it's a loss.
  ranked_reward_alpha: 0.75
  # max/min number of games to consider
  reward_buffer_max_size: 250
  reward_buffer_min_size: 50

  # Learning options:
  # some useful tips for selecting these parameter values:
  # https://stackoverflow.com/a/49924566/7483950
  # learning rate
  lr: 1E-3
  # number times that the learning algorithm will work through the entire training dataset (PSJ -- we actually never want training to stop)
  epochs: 1E6
  # number of batch iterations before a training epoch is considered finished
  steps_per_epoch: 100
  # number of seconds to wait to check if enough games have been played
  game_count_delay: 20
  verbose: 2

  # AlphaZero problem options:
  # max/min number of games to consider (ordered by time) when training the policy
  max_buffer_size: 256
  min_buffer_size: 128
  # number of training examples to use when updating model parameters
  batch_size: 32
  # folder in which to store the trained models
  policy_checkpoint_dir: '/projects/rlmolecule/$USER/stable_radical_opt/policy_checkpoints'

  # MoleculeTFAlphaZeroProblem options:
  # size of network hidden layers
  features: 64
  # number of global state attention heads. Must be a factor of `features`
  num_heads: 4
  # number of message passing layers
  num_messages: 3

# Parameters for running the Monte Carlo Tree Search games
mcts_config:
  # Minimum reward to return for invalid actions
  min_reward: 0
  pbc_c_base: 1.0
  pbc_c_init: 1.25
  # dirichlet 'shape' parameter. Larger values spread out probability over more moves.
  dirichlet_alpha: 1.0
  # percentage to favor dirichlet noise vs. prior estimation. Smaller means less noise
  dirichlet_x: 0.5
  # number of samples to perform at each level of the MCTS search
  num_mcts_samples: 500
  # the maximum search depth
  max_depth: 1000000
  #ucb_constant: math.sqrt(2)

# Database settings for the Object Relational Model (ORM)
# Used to store games and communicate between the policy model (run on GPUs) and rollout (run on CPUs)
sql_database:
  # settings to connect to NREL's yuma database
  drivername: "postgresql+psycopg2"
  dbname: "bde"
  port: "5432"
  host: "yuma.hpc.nrel.gov"
  user: "rlops"
  # read the password from a file
  passwd_file: '/projects/rlmolecule/rlops_pass'
