
run_id: 'logp_example'

# Parameters for setting up the problem
problem_config:
    # maximum number of heavy atoms
    max_atoms: 200
    # minimum number of heavy atoms
    min_atoms: 1
    # atoms to use when building the molecule
    atom_additions: ['C', 'N', 'O']
    # if set, don't construct molecules greater than a given Synthetic Accessibility (SA) score
    # see: https://github.com/rdkit/rdkit/blob/master/Contrib/SA_Score/sascorer.py
    sa_score_threshold: 3.5
    # whether to consider stereoisomers different molecules
    stereoisomers: False
    # try to get a 3D embedding of the molecule, and if this fails, remote it.
    try_embedding: True

# Parameters for training the policy model
train_config:
    # Reward options:
    # if the reward for a given game is > the previous 
    # *ranked_reward_alpha* fraction of games (e.g., 75% of games),
    # then it's a win. Otherwise, it's a loss.
    ranked_reward_alpha: 0.75
    # max/min number of games to consider 
    reward_buffer_max_size: 50
    reward_buffer_min_size: 10
    # option to use a linear bounded reward instead:
    linear_reward: True
    min_reward: 0
    max_reward: 20

    # Learning options:
    # some useful tips for selecting these parameter values:
    # https://stackoverflow.com/a/49924566/7483950
    # learning rate
    lr: 1E-3
    # number times that the learning algorithm will work through the entire training dataset
    epochs: 1E4
    # number of batch iterations before a training epoch is considered finished
    steps_per_epoch: 100
    # number of seconds to wait to check if enough games have been played
    game_count_delay: 20
    verbose: 2

    # AlphaZero problem options:
    # max/min number of games to consider (ordered by time) when training the policy
    max_buffer_size: 200
    min_buffer_size: 15
    # number of training examples to use when updating model parameters
    batch_size: 32
    # folder in which to store the trained models
    policy_checkpoint_dir: '/projects/rlmolecule/$USER/logp/logp_example/policy_checkpoints'

    # MoleculeTFAlphaZeroProblem options:
    # size of network hidden layers
    features: 8
    # number of global state attention heads. Must be a factor of `features`
    num_heads: 2
    # number of message passing layers
    num_messages: 1

# Parameters for running the Monte Carlo Tree Search games
mcts_config:
    # Minimum reward to return for invalid actions
    min_reward: 0
    pbc_c_base: 1.0
    pbc_c_init: 1.25
    # dirichlet 'shape' parameter. Larger values spread out probability over more moves.
    dirichlet_alpha: 1.0
    # percentage to favor dirichlet noise vs. prior estimation. Smaller means less noise
    dirichlet_x: 0.25
    # number of samples to perform at each level of the MCTS search
    num_mcts_samples: 500
    # the maximum search depth
    max_depth: 1000000
    #ucb_constant: math.sqrt(2)

# Database settings for the Object Relational Model (ORM)
# Used to store games and communicate between the policy model (run on GPUs) and rollout (run on CPUs)
sql_database:
   # settings to connect to NREL's yuma database
   drivername: "postgresql+psycopg2"
   dbname: "bde"
   port: "5432"
   host: "yuma.hpc.nrel.gov"
   user: "rlops"
   # read the password from a file
   passwd_file: '/projects/rlmolecule/rlops_pass'
