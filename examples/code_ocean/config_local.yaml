# Config file for stable_radical_opt.py

run_id: 'local_run'

# Parameters for setting up the problem
problem_config:
  initial_state: 'C'
  # maximum number of heavy atoms
  max_atoms: 10
  # minimum number of heavy atoms
  min_atoms: 4
  # atoms to use when building the molecule
  atom_additions: [ 'C', 'N', 'O', 'S' ]
  # if set, don't construct molecules greater than a given Synthetic Accessibility (SA) score
  # see: https://github.com/rdkit/rdkit/blob/master/Contrib/SA_Score/sascorer.py
  sa_score_threshold: 4.0
  # whether to consider stereoisomers different molecules
  stereoisomers: True
  # try to get a 3D embedding of the molecule, and if this fails, remove it.
  try_embedding: False
  canonicalize_tautomers: False  # this is CPU intensive
  cache_dir: 'builder_cache/'
  num_shards: 1
  parallel: False
  redox_model: 'redox-models/models/redox_model'
  stability_model: 'redox-models/models/stability_model'

# Parameters for training the policy model
train_config:
  # Reward options:
  # if the reward for a given game is > the previous
  # *ranked_reward_alpha* fraction of games (e.g., 75% of games),
  # then it's a win. Otherwise, it's a loss.
  ranked_reward_alpha: 0.75
  # max/min number of games to consider
  reward_buffer_max_size: 100
  reward_buffer_min_size: 25

  # Learning options:
  # some useful tips for selecting these parameter values:
  # https://stackoverflow.com/a/49924566/7483950
  # learning rate
  lr: 1E-3
  # number times that the learning algorithm will work through the entire training dataset (PSJ -- we actually never want training to stop)
  epochs: 1E6
  # number of batch iterations before a training epoch is considered finished
  steps_per_epoch: 100
  # number of seconds to wait to check if enough games have been played
  game_count_delay: 20
  verbose: 2

  # AlphaZero problem options:
  # max/min number of games to consider (ordered by time) when training the policy
  max_buffer_size: 256
  min_buffer_size: 32
  # number of training examples to use when updating model parameters
  batch_size: 32
  # folder in which to store the trained models
  policy_checkpoint_dir: 'policy_checkpoints'

  # MoleculeTFAlphaZeroProblem options:
  # size of network hidden layers
  features: 16
  # number of global state attention heads. Must be a factor of `features`
  num_heads: 1
  # number of message passing layers
  num_messages: 1

# Parameters for running the Monte Carlo Tree Search games
mcts_config:
  # Minimum reward to return for invalid actions
  min_reward: 0
  pbc_c_base: 1.0
  pbc_c_init: 1.25
  # dirichlet 'shape' parameter. Larger values spread out probability over more moves.
  dirichlet_alpha: 1.0
  # percentage to favor dirichlet noise vs. prior estimation. Smaller means less noise
  dirichlet_x: 0.5
  # number of samples to perform at each level of the MCTS search
  num_mcts_samples: 100
  # Maximum number of seconds per MCTS round
  timeout: 30
  # the maximum search depth
  max_depth: 1000000
  #ucb_constant: math.sqrt(2)

# Database settings for the Object Relational Model (ORM)
# Used to store games and communicate between the policy model (run on GPUs) and rollout (run on CPUs)
sql_database:
   # settings to connect to NREL's yuma database
   drivername: "postgresql+psycopg2"
   dbname: "rl"
   port: "5432"
   # This will be overwritten by env variable DB_HOST in rlmolecule/sql/run_config.sh
   host: "localhost"
   user: "example_user"
   passwd: "tmppassword"